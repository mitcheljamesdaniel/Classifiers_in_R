---
title: "Classifiers_Tutorial"
author: "Mitchel Daniel"
date: "2023-08-31"
output: html_document
---



#####Consider using We instead of I for the worked example so it feels more like a tutorial


4 Machine Learning Classifiers in R

In this tutorial, I'll discuss 4 widely-used machine learning classifiers and show you how to implement them in R. I'll use a worked example to compare the strengths and weaknesses of each classifier when it comes to flexibility, interpretability, computation time, and visualization options. To keep things concise, I will be light on theory and focus on the practical concerns related to choosing the right classifier and applying it to your dataset.

What is a Machine Learning Classifier?

In machine learning, a classifier is a predictive model that attempts to accurately categorize data into one of multiple classes. Common examples include classifying emails as 'spam' or 'not spam', predicting whether or not a loan is likely to be repaid, or identifying whether or not a patient has a particular disease. The variable being predicted is commonly referred to as the target variable, and the variables being used to predict the variable are the predictors or features.

Many different classification algorithms exist, and there is no robust theory for determining which algorithm will be best for a given problem. Consequently, it is usually best to try out several different classifiers to determine which one performs best. We will also discuss some attributes of commonly used classifiers that can help in deciding which ones are worth trying for a given use case.

To determine which classifier performs best, classification accuracy (i.e. the proportion of cases that are accurately categorized by the model) is a great place to start. However, note that accuracy can be misleading if your dataset is imbalanced (i.e. if certain classes are car more common than others). If you are dealing with an imbalanced dataset, check out <link to repo with imbalanced learning tutorial> to learn about alternative to accuracy that more accurately quantify model performance, and several techniques for preventing the bias that results from training a classifier on an imbalanced dataset.

Binary Classification

In this tutorial, I will focus on binary classification, which is when the target variable has two possible classes. Multi-class problems, in which there are three or more classes, are also common. My tutorial on imbalanced learning <link> gives some examples of how to perform multi-class classification in R.

In binary classification, one class is considered the 'normal' state and assigned a value of 0; the other class is considered the 'abnormal' state and assigned a value of 1. Generally, classifiers use the Bernoulli probability distribution, which represents the situation where an event has a binary outcome, 0 or 1. 

<have equation for Bernoulli probability function that uses q and p>

In essence, this means that the model predicts the probability (p) that a given case belongs to the abnormal state, which in turn determines the probability that the case belongs to the normal state (q = 1 - p).

```{r setup, include=FALSE}
####Do i need this code chunk for a tutorial?
library(knitr)
library(rmarkdown)
knitr::opts_chunk$set(echo = TRUE)
```

Using Loan Approvals to Explore Classifiers

For this tutorial, we will use the Loan Approval Prediction dataset provided by Kai on Kaggle.com <https://www.kaggle.com/datasets/architsharma01/loan-approval-prediction-dataset>. With this dataset, we can use several different classifiers to predict whether or not loans were approved based on financial records and associated information that is commonly used to determine the eligibility of individuals or organizations for loans. The dataset takes just a tiny bit of cleaning before it is ready to use.

```{r read_in_and_clean_data}

library(readr)
library(dplyr)

#read in the dataset as a data frame
loan_data <- as.data.frame(read_csv("loan_approval_dataset.csv"))

#data is already tidy

#there are no duplicated cases in this dataset
#sum(duplicated(loan_data))

#there are no missing data
#anyDuplicated(loan_data)

factors_vector <- c("education", "self_employed", "loan_status")

for(i in factors_vector){
  loan_data[[i]] = as.factor(loan_data[[i]])
}

#loan_id is an arbitrary key that is not useful for any of the analyses, so I drop that column
loan_data <- loan_data %>%
  mutate(loan_id = NULL)

```

Next, I visualize the distribution of the target variable to see how many observations are in each class. There are more approved loans (2656) than rejected loans (1613) in our dataset; however, the difference in size between the two classes is mild enough that I won't worry about using techniques to correct for the imbalance.

``` {r visualize_target_variable}

library(ggplot2)

#visualize distribution of target variable
ggplot(loan_data, aes(x = loan_status, fill = loan_status)) +
  geom_bar() +
  labs(x = 'loan status') +
  scale_x_discrete(labels=c('approved', 'rejected')) +
  theme_classic() +
  theme(legend.position = "none")

#get number of cases in each class
# library(dplyr)
# loan_data %>%
#   group_by(loan_status) %>%
#   summarize(count = n())

```

Next, I examine the relationships between each pair of features. As you can see in the correlogram, annual income is strongly correlated (r > 0.9) with both loan amount and luxury assets value. I exclude this redundant variable to enhance model accuracy and reduce computation time.

``` {r feature selection}

library(psych)
#plot correlogram, leaving out loan_id because it contains arbitrary labels, and loan_status because that's our target variable
pairs.panels(loan_data[,1:11], 
             method = "pearson", #use pearson correlation method
             hist.col = "#00AFBB",
             density = FALSE,  #don't show density plots
             ellipses = FALSE, #don't show correlation ellipses
             smooth = FALSE, #don't show loess line
             cex.cor = 1.5) #increase size of correlation text

#drop income_annum from the data frame
loan_data_reduced <- loan_data %>%
  mutate(income_annum = NULL)

```

The dataset is now ready to analyze, so let's dive into the first classifier.


Logistic Regression

How does logistic regression work?

Logistic regression estimates the probability that a given case belongs to each class by fitting a sigmoid function - i.e. an S-curve - to the data. A logistic regression model is fit by tuning the coefficients for each term in the model, which are log odds (the probability of an abnormal event divided by the probability of a normal event). Once the optimal coefficients are found, they can be used to calculate the conditional probabilities of an abnormal event for a given case. Cases with a probability less than 0.5 are predicted to be 0 ('normal'), and those above 0.5 are predicted to be 1 ('abnormal').

When is logistic regression useful?

Logistic regression is used for binary classification, though methods exist for multinomial regression (which is suitable for 3 or more unordered classes) and ordinal regression (3 or more ordered classes). Logistic regression is relatively flexible in that can handle both continuous and categorical features, and does not make any assumptions about the distribution of the features. Because logistic regression models are relatively simple, they are easy to implement, faster to run, less prone to overfitting, and more straightforward to interpret compared to many other classifiers.

Logistic regression performs well with datasets that are linearly separable, meaning that it is possible to draw a line within the multidimensional space of the features that perfectly splits the two classes from one another. However, real-world datasets are often noisy, meaning that linearly separable data is uncommon. Logistic regression also assumes that independent variables are linearly related to the log odds, so more complex relationships can result in poor model performance.

Implementing logistic regression

I split the data into training (80%) and testing (20%) sets.

``` {r partition_data}

#set the seed so that data partitioning is reproducible
seed = 437
set.seed(seed)

#create an index and use it to randomly subset the data into testing and training sets
partition_ind <- sample(2, nrow(loan_data_reduced), replace = TRUE, prob = c(0.8, 0.2)) #Note that while the indices are sampled with replacement, cases are ultimately sampled without replacement (i.e. each case is placed in either the training or testing set, never both)
loan_train <- loan_data_reduced[partition_ind == 1,]
loan_test <- loan_data_reduced[partition_ind == 2,] 

```

Any of the features that remain in our dataset could be informative about the target variable, so I include them all in the initial model. Additionally, interactions between any of the features could be informative, so I include two-way interaction terms. To keeps things relatively simple for this tutorial, I will not include higher-order interactions. Including higher-order interactions can increase the model's performance, but also increases computation time and reduces interpretability. Additionally, increasing the number of terms in your model above roughly one tenth the number of cases is not recommended because the risk of overfitting becomes much higher. 

We have 11 features, so including the intercept, all main effects, and all 2-way interactions gives us 11 + (10 choose 2) = 56 coefficients in our model, which keeps the ratio of coefficients to cases (56:3455) well below the 1:10 rule-of-thumb cut-off.

``` {r }

log_reg <- glm(loan_status ~ (no_of_dependents + education + self_employed + loan_amount + loan_term + 
                              cibil_score + residential_assets_value + commercial_assets_value + luxury_assets_value +
                              bank_asset_value)^2, 
               data = loan_train, 
               family = "binomial")

# log_reg_basic <- glm(loan_status ~ loan_id + no_of_dependents + education + self_employed + loan_amount + loan_term + cibil_score + residential_assets_value + commercial_assets_value + luxury_assets_value + bank_asset_value, data = loan_data, family = "binomial")

# log_reg_full <- glm(loan_status ~ loan_id * no_of_dependents * education * self_employed * loan_amount * loan_term * cibil_score * residential_assets_value * commercial_assets_value * luxury_assets_value * bank_asset_value, data = loan_data, family = "binomial", control = list(maxit = 50, trace=TRUE))
```
Running the model throws the warning "fitted probabilities numerically 0 or 1 occurred". This warning means that there is complete separation: some values of certain features in our model perfectly predict whether the loan was approved or rejected, resulting in conditional probabilities of 0 or 1. This isn't necessarily a bad thing - in fact, complete separation often arises when the model is really good at predicting the target variable. However, when there is complete separation, Wald tests and likelihood ratio tests will give biased results. AIC is still reliable, so we will use AIC for backward stepwise model reduction.

Model reduction is a large and complex topic unto itself, but in essence backward stepwise model reduction starts with the full model (or a model containing as many terms as is feasible), removing the term that leads to the greatest improvement in the model, and repeating this until removing any term no longer significantly improves the model. In this case, I use AIC as the measure of model performance, which is an estimate of prediction error that accounts for the trade-off between model accuracy and model simplicity. A smaller AIC suggests a better model. I use the stepAIC() functions from package MASS to automate backward stepwise model selection.

``` {r model_reduction}

library(MASS)

#perform backwards stepwise model reduction by AIC, and get number of coefficients in the final model
reduced_log_reg <- stepAIC(log_reg)
length(reduced_log_reg$coefficients)

```

I've obtained a much-reduced model with a total of `r length(reduced_log_reg$coefficients)` coefficients. Stepwise model reduction by AIC eliminates terms that do not significantly improve the model. This means that we can examine which features (and interaction terms) remain to learn which variables are informative about the target variable.

Validating logistic regression

Before we start interpreting the model, we should examine how well it performs on the testing set. Using the confusionMatrix() function from the caret package, we can calculate a wide range of commonly-used performance metrics. Our model performs very well according to all these metrics. In the real-world, it is generally a good idea to train and test your model multiple times on different partitionings of the data, to ensure that your model performs well no matter which cases happened to end up in the training and testing set. However, for the purposes of this tutorial, we will stick to using one training and testing set.

``` {r validate logistic regression}

library(insight)
library(caret)

#get predictions for the testing set
log_predictions <- get_predicted(reduced_log_reg, loan_test)
#convert probabilities into binary expectations
predictions_status <- as.factor(ifelse(log_predictions >= 0.5, "Rejected", "Approved"))
#calculate model performance metrics
caret::confusionMatrix(predictions_status, loan_test$loan_status, mode = "everything", positive = "Rejected")

```


Interpreting logistic regression

I convert the coefficients from log odds to (proportional) odds ratios, which is more intuitive to interpret. For every one unit increase in a feature, the probability that the case is in the abnormal class increases by a factor equal to the odds ratio. For example, according to the model, increasing the loan term by 1 year increases the probability that the loan will be rejected by `r round(head(sort(odds_ratios, decreasing = TRUE))[1], 2)` times, all else being equal. 

The fact that logistic regression calculates a log odds for each term in the model makes interpretation relatively straightforward. All else being equal, the bigger the lod odds ratio, the more important the term (though don't forget to account for differences in the range - more on this below). Logistic regression calculates a single odds ratio for each term because it assumes that the relationship between each feature and the log odds is linear. In other words, logistic regression assumes that the data are best represented by a logistic curve. So, logistic regression will perform poorly if the relationship between features and the log odds of the target variable is non-linear.

``` {r obtain_log_odds}

#convert log odds to odds ratios
odds_ratios <- exp(reduced_log_reg$coefficients)
#print terms with the 6 highest odds ratios
odds_ratios

```

Visualizing logistic regression

Since our model has many features, we may want a way to visualize the relationship between one or two features and the target variable, while holding other variables constant. Fortunately, logistic regression is straightforward to visualize because we can use the model to calculate the probability of different classes for a given input.

A common method for obtaining probabilities from a logistic regression model is to use data_grid() and add_predictions() from the modelr package. However, we have too many unique value combinations in our features for this method to work. Instead, we can use the visualisation_matrix() function from package modelbased to generate a grid of values for one or more features, and the get_predicted() function from package insight to calculate the model's predictions for that grid of values. The predictions are the probability that the loan is rejected, which are easy to interpret. Since we are ultimately interested in predicting a binary outcome, I convert the probabilities into binary expectations ("approval" or "rejection"). Then, we can clearly plot the relationship between the feature of interest, the predicted probability, and the expected loan status.

In the plot below we can see that CIBIL score, which is a summary of a person's credit history, unsurprisingly has a very large impact on the probability that a loan will be approved. You'll notice that CIBIL score has a relatively small odds ratio in our model (`r odds_ratios[6]`), yet a big impact on the predictions. This is because CIBIL score has a large range (`r range(loan_train$cibil_score)`), and the odds ratio describes the factor by which the model's prediction changes for each unit increase in the feature. This is one reason why it's helpful to consider the range (and to visualize the feature) when interpreting the odds ratio.

``` {r visualize single feature predictions}

library(modelbased)

#generate grid of evenly-spaced values for a feature of interest, with the mean value for every other feature
my_grid <- loan_train[1:10] %>%
  visualisation_matrix("cibil_score", length = 600, factors = "median", numerics = "median")
#obtain predictions for each row in the grid
cibil_score_predictions <- get_predicted(reduced_log_reg, my_grid, predict = "expectation")
#combine grid of cibil scores and predictions into one dataframe
grid_and_predictions <- as.data.frame(cbind(my_grid$cibil_score, cibil_score_predictions))
colnames(grid_and_predictions) <- c("cibil_score", "predicted_probability_of_rejection")
#convert probabilities into binary expectations
grid_predictions_status <- grid_and_predictions %>%
  mutate(predicted_loan_status = case_when(predicted_probability_of_rejection >= 0.5 ~ "rejected",
                                           predicted_probability_of_rejection < 0.5 ~ "approved"))

#plot conditional probability of loan rejection for each row in the grid
grid_predictions_status %>%
  ggplot(aes(x = cibil_score, y = predicted_probability_of_rejection, color = predicted_loan_status)) +
  geom_point(size = 1.5) +
  labs(x = 'cibil score',
       y = 'predicted probability of rejection') +
  guides(color=guide_legend(title="predicted loan status")) +
  theme_classic()
```

We can also visualize the joint effect of two or more features on the model's predictions, which is useful for features like education and loan term that interact with one another. Visualizing the effect of these variables reveals that being a graduate increases the odds of rejection for a short-term loan, but decreases the odds of rejection for a long-term loan. Note that in this figure the probability of loan rejection is always lower than the average in our dataset, and the curves appear exponential rather than logistic. This is because we are only visualizing the effects of these two features at the median value of all the other features in our dataset. As this example illustrates, it's important to interpret visualizations of 1 or two features with a grain of salt when dealing with a multivariate dataset.

``` {r visualize two features}

#generate grid for two features
two_grid <- loan_train[1:10] %>%
  visualisation_matrix(c("loan_term", "education"), length = 50, factors = "median", numerics = "median")
#obtain predictions
term_education_predictions <- get_predicted(reduced_log_reg, two_grid, predict = "expectation")
#combine grid and predictions into one dataframe
two_grid_and_predictions <- as.data.frame(cbind(two_grid$loan_term, two_grid$education, term_education_predictions))
colnames(two_grid_and_predictions) <- c("loan_term", "education", "predicted_probability_of_rejection")
two_grid_and_predictions$education <- as.factor(two_grid_and_predictions$education)

#plot conditional probability of loan rejection for both features
two_grid_and_predictions %>%
  ggplot(aes(x = loan_term, y = predicted_probability_of_rejection, group = education, color = education)) +
  geom_line(linewidth = 1.5) +
  labs(x = 'loan term',
       y = 'predicted probability of rejection') +
  guides(color=guide_legend(title="education")) +
  scale_color_manual(labels = c("Graduate", "Not graduate"),
                     values = c("green", "brown")) +
  theme_classic()

```


Random Forest

How does random forest work?

To understand random forest, we first need to understand decision trees. A decision tree is a hierarchical model - like a flowchart - in which the cases in the dataset are divided into smaller and smaller groups as efficiently as is possible using consecutive, binary criteria. If used for classification, each leaf node (i.e. the tips of the tree) typically constitutes a sub-group of cases that all, or mostly, belong to a single class of the target variable. A decision tree can be used to predict class membership for a given case by following the decisions through the tree until reaching a leaf node, and assigning the case the class label of that leaf node.

As the name suggests, a random forest consists of multiple decision trees. A random forest predicts class membership by aggregating the predictions of individual trees (usually by assigning the class label predicted that is predicted by the most trees). When fitting each tree, data are sampled from the training set with replacement so that the data used for each tree are not identical. This sampling process is called "bagging". Random forest also uses feature randomness, meaning that only a random subset of the features in the dataset are eligible when determining how to optimally split the data for a given tree. Bagging and feature randomness help to ensure that trees are uncorrelated with one another. This is important because decision trees are prone to overfitting. By aggregating many, uncorrelated trees, random forest dramatically reduces the risk of overfitting. Note that bagging is less effective at reducing overfitting if cases are correlated with one another (e.g. if multiple measurements are taken on the same individual). In our scenario, each case is a loan application from a different individual, so we don't have to worry about correlated cases.

When is random forest useful?

Random forest can be used for both classification and regression tasks, though I focus just on classification here. Random forest is an extremely versatile machine learning technique because it can handle any combination of continuous and quantitative features, and it makes no assumptions about the distribution of the features, the target variable, or their relationships with one another. Additionally, the use bagging and aggregating uncorrelated trees make random forest less prone to overfitting or bias compared to most other machine learning methods. Random forest does not 

The primary draw-back of random forest is lack of interpretability. The model is really a set of decision trees, and so it is very difficult to summarize a random forest in an intuitive way. If the goal is simply to produce a model that can be used to generate robust predictions, random forest may be a great option. However, if the goal is to understand the importance of different features and how they interact to affect class, then other machine learning techniques will be more useful. 


Implementing random forest

Several hyperparameters are set by the data scientist, which control how the random forest learns. We can set the minimum size of leaf (terminal) nodes. A higher value makes the random forest run faster, but tends to reduce model performance. We can determine the number of trees. More trees takes longer, but leads to less error and less overfitting. We can also set the number of features sampled per tree. Too few candidate features means that the model has low accuracy; too many makes the trees similar to one another, increasing the risk of overfitting.

In our case, the dataset is small enough that computation time is not a concern, so we will use the default minimum leaf node size of 1, and set the number of trees to 10000. We will follow the general rule-of-thumb suggested by Mueller and Guido <cite "Introduction to Machine Learning with Python"> and use the square root of the total number of features in our dataset. In a real-world use case, if small differences in model performance are important then it is worth fine-tuning these hyperparameters to optimize the model.

The logistic regression showed that interactions between our features are important for predicting loan status. However, with random forest, there is no need to explicitly encode interaction terms into the model, since interactions between features are inherently represented in hierarchical decision trees. 

I use the randomForest() function from the randomForest() package. This functions trains the random forest and computes the predictions all in one step.

``` {r train random forest}

library(randomForest)

#random forest takes the features and target as separate inputs, so we split them into different objects here
x_train <- loan_train[,1:10]
y_train <- loan_train[,11]
x_test <- loan_test[,1:10]
y_test <- loan_test[,11]
#run random forest with 10000 trees, minimum node size of 1, and number of sampled features equal to sqrt(p) where p is number of features
rf <- randomForest(x_train, y = y_train, xtest = x_test, ytest = y_test, ntree = 10000)

```


Now that we've fit the random forest, let's see how well it performed. Looking at the confusion matrix and performance metrics, we can see that the random forest performed extremely well.

Is is easy to examine how the number of trees in a random forest affect the model's performance. We can plot the error rate against the number of trees used in the random forest to see whether we should alter this hyperparameter to achieve better performance. In this case, the error rate plateaus at a low level after just a few hundred trees, indicating that 10,000 tree is more than enough.

``` {r validate random forest}

#get confusion matrix and calculate model performance metrics
caret::confusionMatrix(rf$test$predicted, y_test, mode = "everything", positive = "Rejected")

plot(x = 1:rf$ntree, y = rf$err.rate[,1], col = "black", type = "l", xlab = "number of tree", ylab = "test error rate", ylim = c(0,0.15))
lines(1:rf$ntree, rf$err.rate[,2], col = "red", type = "l")
lines(1:rf$ntree, rf$err.rate[,3], col = "blue", type = "l")
legend("topright", c("overall", "approved", "rejected"), col = c("black", "red", "blue"), cex = 1, lty = 1)

```

Now that we know the random forest performed well, we can interpret the results. The Gini importance index is a measure of how many times a feature is used to split a node, weighted by the number of samples that it splits. Features with a higher Gini importance index are more informative about the target variable. We can obtain the Gini indices directly, and the randomForest package provides a convenient way to visually compare the indices.

In our random forest, CIBIL score is far and away the most important feature.

``` {r interpret random forest}

#print the Gini importance indices for each feature
rf$importance
#plot the Gini importance indices
varImpPlot(rf)

```

It is difficult to understand or visualize the effect of features (or their interactions) on the target variable beyond measures of feature importance, which is one of the main limitations of random forest.


Neural Network

How does a neural network work?

A neural network consists of multiple, sequential layers of interconnected nodes. An input layer receives the raw inputs (i.e. the features), the hidden layer(s) process the data, and the output layer produces the predictions. There a several general types of neural networks. I will focus on feedforward neural networks, also known as multilayer perceptrons, because this is one of the simplest and most widely used types of neural network.

In a feedforward neural network, Generally, the inputs are values of the features included in the analysis, with one input node for each feature. Inputs must all be numeric, so any categorical features must be encoded numerically. For a classification problem, there is one output node for each class. The number of hidden layers, and the number of nodes in each hidden layer, are set by the user. 

Each node in one layer is connected to every node in the next layer by edges. The inputs are passed forward from the input layer to next layer via the edges, after being multipled by the weight associated with each edge. Each node in the next layer takes, as its input, the sum of the weighted values passed to it from the first layer. This process is repeated for each layer. Once the data reach the output layer, the data in each node are assigned a value of 0 or 1 depending on whether they exceed the output node's threshold. Alternatively, the output layer can produce probabilities. Each output node is mapped to the a level of the response variable that it's outcomes are most closely correlated with, and then the weights and thresholds are tuned using gradient descent to optimize the model's performance. In modern feedforward neural networks, training is usually performed using backpropagation, meaning that information flows backwards allowing weights to be adjusted based on the values produced by the output layer.

When is a neural network useful?

In principle, neural networks can approximate any function, making them extremely versatile. This flexibility is part of the reason that neural networks are frequently used for complex problems such as computer vision, natural language processing, and pattern recognition. In practice, neural networks can effectively model any pattern provided sufficient data and network complexity. Neural networks with multiple layers are effective at classifying data that is not linearly seperable.

The main drawbacks of neural networks are that they tend to be slower than other machine learning techniques, have a greater risk of overfitting, and are very difficult to interpret. For more complex problems, figuring out the best network architecture can be something of an art.

Implementing a neural network

Before implementing a neural network, I normalize the features in the dataset. Since the features are measured on wildly different scales, normalizing them will improve the speed and success rate of gradient descent. To normalize our categorical features, we must first convert them to type numeric. Our categorical features have just two levels each, so we can simply use as.numeric() to convert them. The neural network function we will use requires the target variable to encoded as dummy variables (i.e. one column indicating whether or not the case belongs to a given class). We do this encoding using RSNNS::decodeClassLabels().

``` {r normalize features}

library(datawizard)
library(RSNNS)

#create new data frame with all cases, which we will normalize
loan_data_to_norm <- loan_data_reduced
#convert categorical features to numeric
loan_data_to_norm$education <- as.numeric(loan_data_norm$education)
loan_data_to_norm$self_employed <- as.numeric(loan_data_norm$self_employed)
#normalize features
loan_data_norm <- as.data.frame(apply(loan_data_norm[,1:10], 2, FUN = normalize))
#give data frame name specific to neural network, since we will be using a different normalized data frame for the next classifier
loan_data_nn <- loan_data_norm
loan_data_nn[c("approved", "rejected")] <- decodeClassLabels(loan_data_reduced$loan_status)

```

I partition the normalized dataset using the same index as before so that the neural network is trained on the same data as the other machine learning models.

``` {r partition data for neural network}

#partition the normalized dataset for the neural network, using the same seed as before
loan_nn_train <- loan_data_nn[partition_ind == 1,]
loan_nn_test <- loan_data_nn[partition_ind == 2,] 
#separate the the partitioned normalized dataset into features and target variable
x_train_nn <- loan_nn_train[,1:10]
y_train_nn <- loan_nn_train[,11:12]
x_test_nn <- loan_nn_test[,1:10]
y_test_nn <- loan_nn_test[,11:12]

```

Two important hyperparameters have a large impact on model performance and speed: the number of hidden layers, and the number of nodes per hidden layer. Too few nodes results in underfitting (i.e. failure to detect patterns in the data). Adding nodes increases computation, and too many nodes leads to overfitting.

There is no foolproof way to determine in advance the optimal number of hidden layers and nodes. One or two hidden layers is sufficient for most relatively simple classification problems, and most rules of thumb place the optimal number of nodes per layer below the number of input nodes <cite>. So, one common strategy is to start with more nodes in the hidden layer than there are input nodes, and then use a pruning algorithm to get down to an optimum. Tree pruning consists of removing nodes that have little effect (typically nodes on either end of edges that have weights close to 0).

We will use the mlp() function from the RSNNS package because it provides automated neural network pruning.

``` {r implement neural network}

library(RSNNS)

#set the pruning algorithm parameters
pruneFuncParams <- list(max_pr_error_increase = 10.0,
                        pr_accepted_error = 1.0,
                        no_of_pr_retrain_cycles = 1000, 
                        min_error_to_stop = 0.01,
                        init_matrix_value = 1e-6,
                        input_pruning = TRUE,
                        hidden_pruning = TRUE)
#train the neural network
neural_mod = mlp(x_train_nn, 
       y_train_nn, 
       size = 13, #set number of hidden nodes greater than number of input variables
       learnFuncParams = 0.1,
       maxit = 10000, #set maxium number of iterations
       learnFunc = "Std_Backpropagation",
       inputsTest = x_test_nn, 
       targetsTest = y_test_nn,
       pruneFunc = "OptimalBrainSurgeon",
       pruneFuncParams = pruneFuncParams)

```

mlp() outputs two probabilities per case, one for each of the classes of our target variable. To convert these probabilities into predictions, we can determine which of the two classes has the higher probability for each case. We can see that the neural network performed very well by all metrics.

We can also plot the error for each iteration of the neural network to check whether we used enough iterations to optimize the network. Both the fit and test error plateaus well before the maximum number of iterations, indicating that 10000 iterations was more than sufficient. In contrast, if the error was "spikey" throughout, this would indicate it did not have sufficient time to converge to an optimum. If the fit error plateaued and then subsequently declined again, this would be an indication of overfitting.

``` {r validate neural network}

#obtain predictions
neural_pred <- predict(neural_mod, x_test_norm)
#determine the most strongly predicted classification for each observation
pred_labelled <- max.col(neural_pred)
#get the correct classification for each case in the testing set
targets_labelled <- max.col(y_test_norm)
#convert target variable from numeric back to factor
pred_labelled <- as.factor(ifelse(pred_labelled == 1 , "approved", "rejected"))
targets_labelled <- as.factor(ifelse(targets_labelled == 1 , "approved", "rejected"))
#generate confusion matrix
caret::confusionMatrix(as.factor(pred_labelled), as.factor(targets_labelled), mode = "everything", positive = "rejected")

#plot iterative error (fit error in black, test error in red)
plotIterativeError(neural_mod)

```
At the time of writing this post, there is no straightforward way to visualize a pruned neural network produced by mlp(). However, we can fit a new neural network using the same number of hidden nodes as the pruned network, and visualize that. Visualizing the neural network can allow us to see how different input variables combine to affect the probability of each output. Note, however, that for complex neural networks these diagrams are often too complex to be very enlightening.

We can use the weightMatrix() function to see the weights assigned to each edge. Hidden nodes that don't appear in the matrix were pruned.

``` {r visualize neural network}

library(NeuralNetTools)

#see edge weights and number of remaining nodes
weightMatrix(neural_mod)

#train neural network with 12 hidden nodes
neural_mod_2 = mlp(x_train_nn, 
       y_train_nn, 
       size = 12, 
       learnFuncParams = c(0.1),
       maxit = 10000, 
       learnFunc = "Std_Backpropagation",
       inputsTest = x_test_nn, 
       targetsTest = y_test_nn)
#plot the neural network, with positive weights in blue and negative weights in red
plotnet(neural_mod_2, pos_col = "blue", neg_col = "red")

```

Support Vector Machines

How does Support Vector Machine work?

Support Vector Machine (SVM) attempts to find the hyperplane in the multi-dimensional space of your features that best divides the different classes of your target variable. "Best" means the hyperplane that has the largest gap between the two classes, called the margin. The hyperplane is a decision boundary, so which side of the boundary that a point falls on determines its predicted classification. Finding a hyperplane with a larger margin leaves more room for error, meaning that points can be classified with greater accuracy.

Support vectors are the points in each class that are closest to the hyperplane, which determine the position and orientation of the hyperplane. (The distances from the support vectors and the hyperplane are what is maximized when tuning the model.) If the data are not linearly separable, the data are transformed to a higher dimension using a kernel function to obtain a linear boundary.


When is SVM useful?

SVM is a relatively simple algorithm, which makes it easy to implement and quick to run compare to many other machine learning techniques. When making predictions, SVM is quite memory-efficient because it only needs to keep a subset of the data (the support vectors) in memory. As a result, SVM handles data with high dimensionality well. The kernel trick means that SVM performs well on data that are not linearly separable, making it a useful technique for most real-world datasets. 

The kernel trick makes the model coefficients very difficult to interpret, so models fit to data that are not linearly separable are essentially black boxes. SVM is highly sensitive to the kernel parameters and cost function used, so model tuning can be somewhat time-consuming. SVM can generate classification predictions, but not probabilities (although the distance of a point from the decision boundary can sometimes be used as a proxy for prediction confidence).


Implementing SVM

SVM fits the hyperplane by assessing the distances among points in Euclidean space. So, we want to use the normalized variables to prevent variables measured on a larger scale from dominating the model. Like the neural network, the SVM function we will use cannot take factors as inputs, so we will used the normalized features. If using SVM for classification, the target variable should be a factor, and should not be encoded in dummy variables, so we we will add the raw, target variable to the dataframe with the normalized features.

``` {r normalizing data for svm}

# #create new data frame only numerical features
# loan_data_num <- loan_data_reduced %>%
#   mutate(education = NULL,
#          self_employed = NULL,
#          loan_status = NULL)
# #normalize numerical features
# loan_data_num <- as.data.frame(apply(loan_data_num[,1:8], 2, FUN = normalize))
# #add categorical variables to normalized dataframe
# loan_data_svm <- cbind(loan_data_num, loan_data_reduced[,2:3], loan_data_reduced[11])

#give normalized data frame a SVM-specific name
loan_data_svm <- loan_data_norm
#add target variable as type fact to the svm-specific data frame
loan_data_svm['loan_status'] <- loan_data_reduced[11]

```

Next, I partition the normalized data frame for the SVM.

``` {r partitioning normalized data frame for SVM}

#partition the normalized dataset, using the same seed as before
loan_svm_train <- loan_data_svm[partition_ind == 1,]
loan_svm_test <- loan_data_svm[partition_ind == 2,] 

```

To implement SVM, I use the tune() function in package e1071, which supports automated tuning of the cost parameter and the gamma parameter that controls the kernel function.

``` {r training SVM}

library(e1071)

tuned_svm <- tune(svm, loan_svm_train[,1:10], train.y = loan_svm_train[,11], kernel = "radial",
                  ranges = list(cost = c(0.1, 1, 5, 10, 50, 100, 1000, 10000),
                  gamma = c(0.001, 0.01, 0.05, 0.1, 0.25, 0.5, 1, 2, 3, 4)))

```

The tune() function trains multiple SVMs using every combination of the parameter values provided by the user. It then determines the best model. We can then use the best model for validation, predictions, and visualizations.

By looking at the best model, We can see which gamma and cost values performed best. Since best values are not at either end of the range that we considered, the parameter values in our best model are probably close to the optimal values. In a use case in which small differences in model performance matter, it would be worth re-running tune with a more fine-grained set of parameter values around those in our best model to more precisely fine-tune the SVM.

If the best parameters were at either extreme of the range we considered, then the optimal values might be far away from what was used in our best model. In that situation, we would want to re-run tune() with the range of parameter values accordingly.

Examining the confusion matrix, we can see that our SVM performed very well by all metrics.

``` {r validating SVM}

tuned_svm$best.parameters


#get svm predictions for the testing set
svm_pred <- predict(tuned_svm$best.model, loan_svm_test[,1:10])

caret::confusionMatrix(svm_pred, loan_svm_test$loan_status, mode = "everything", positive = "Rejected")

```
Conclusion

We have worked through an overview of the advantages and limitations of 4 commonly-used classifiers, including when each technique is useful, and how you can implement, interpret, and visualize the different models. When time allows, it is often best to identify multiple classifiers that seem suitable for your problem, try them out, and compare their performances. I hope this information is helpful in deciding the right technique for your own data science problems.

